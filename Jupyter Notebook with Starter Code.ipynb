{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkvLfqg_Igym"
      },
      "source": [
        "# COS598I Spring 2024: Responsible AI in Societal Deployments\n",
        "**Authors:** Lelia Marie Hampton, Prof. Marzyeh Ghassemi\n",
        "\n",
        "Assignment adapted from MIT 6.882: Ethical Machine Learning in Human Deployments\n"
      ],
      "id": "SkvLfqg_Igym"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLBLWHjIIgyp"
      },
      "source": [
        "## Assignment 2: Algorithmic Fairness Exploration"
      ],
      "id": "kLBLWHjIIgyp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZO24K6BIgyq"
      },
      "source": [
        "#### Guidelines\n",
        "\n",
        "Add helper functions as needed. **Do NOT use outside libraries** such as AI Fairness 360 Toolkit or What-If Toolkit. It is important to learn the mechanics of the underlying approaches.\n",
        "\n",
        "Please include comments (both block comments and inline comments) so that we can easily understand what you're doing.\n",
        "\n",
        "#### Resources\n",
        "\n",
        "[Fairness Definitions Explained](https://fairware.cs.umass.edu/papers/Verma.pdf)\n",
        "\n",
        "[Taiwan Default of Credit Card Clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)\n",
        "\n",
        "- [Paper](https://bradzzz.gitbooks.io/ga-dsi-seattle/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf) See Section 3.1 for description of the dataset features\n",
        "\n",
        "- [Dataset](https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip) You can download the dataset from this URL or from the Ed Resources Tab\n",
        "\n",
        "[Machine Learning Glossary: Fairness](https://developers.google.com/machine-learning/glossary/fairness)\n",
        "\n",
        "[Chapter 4. Fairness Pre-Processing](https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html)"
      ],
      "id": "KZO24K6BIgyq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ItWUWfZFIgyr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict, RepeatedStratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "ItWUWfZFIgyr"
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing code for Taiwan Default dataset\n",
        "\n",
        "def load_dataset(filename='default of credit card clients.xls'):\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(filename, header=1)\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "    numerical_features = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
        "                          'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "\n",
        "    # Separate features and target variable\n",
        "    X = df.drop(columns=['ID', 'default payment next month'])\n",
        "    y = df['default payment next month']\n",
        "\n",
        "    # Preprocessing: One-hot encoding for categorical variables and scaling for numerical variables\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Extract gender as a binary attribute before any transformations\n",
        "    gender_binary = df['SEX'].apply(lambda x: 1 if x == 2 else 0)\n",
        "\n",
        "    # Split the dataset into training and testing sets with a fixed random state for reproducibility\n",
        "    X_train_full, X_test_full, y_train, y_test, gender_train, gender_test = train_test_split(X, y, gender_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Apply preprocessing to training and testing set separately\n",
        "    X_train_processed = preprocessor.fit_transform(X_train_full)\n",
        "    X_test_processed = preprocessor.transform(X_test_full)\n",
        "\n",
        "    # Return processed training and testing sets along with gender attributes\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, gender_train.values, gender_test.values"
      ],
      "metadata": {
        "id": "tk24Xq0LWuEo"
      },
      "id": "tk24Xq0LWuEo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_model(X_train, X_test, y_train, weights=None):\n",
        "\n",
        "    # Initialize the Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=10000, random_state=0)\n",
        "\n",
        "    # Train the Logistic Regression model\n",
        "    model.fit(X_train, y_train, sample_weight=weights)\n",
        "\n",
        "    # Predict on the testing set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    return y_pred, y_pred_proba"
      ],
      "metadata": {
        "id": "OJ7itSmHWt0e"
      },
      "id": "OJ7itSmHWt0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess and load the data\n",
        "X_train, X_test, y_train, y_test, gender_train, gender_test = load_dataset()\n",
        "\n",
        "# train a model and obtain predictions on the test set\n",
        "y_pred, y_pred_proba = train_and_predict_model(X_train, X_test, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print('Accuracy:', accuracy, '\\nAUC:', auc)"
      ],
      "metadata": {
        "id": "EnOBmc7kYHBa"
      },
      "id": "EnOBmc7kYHBa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RYga00d2Igyu"
      },
      "outputs": [],
      "source": [
        "def determine_confusion_matrix(df):\n",
        "    if df['y_true'] == df['y_pred'] == 1:\n",
        "        return 'TP'\n",
        "    elif df['y_pred'] == 1 and df['y_true'] != df['y_pred']:\n",
        "        return 'FP'\n",
        "    elif df['y_true'] == df['y_pred'] == 0:\n",
        "        return 'TN'\n",
        "    else:\n",
        "        return 'FN'"
      ],
      "id": "RYga00d2Igyu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I_bzHiL1Igyu"
      },
      "outputs": [],
      "source": [
        "# Female = 1 and Male = 0\n",
        "fair_df = pd.DataFrame({'sex': gender_test, 'y_true': y_test, 'y_pred': y_pred})\n",
        "fair_df['confusion_matrix'] = fair_df[['y_true','y_pred']].apply(determine_confusion_matrix, axis=1)"
      ],
      "id": "I_bzHiL1Igyu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q4uQAbctIgyu"
      },
      "outputs": [],
      "source": [
        "fair_df.head()"
      ],
      "id": "Q4uQAbctIgyu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YB8dReguIgyv"
      },
      "outputs": [],
      "source": [
        "fair_df['sex'].value_counts()"
      ],
      "id": "YB8dReguIgyv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ7MO-ujIgyv"
      },
      "source": [
        "## Q1) Detecting Algorithmic Bias through Fairness Measurements and Definitions\n",
        "Fill in the code for the fairness definitions."
      ],
      "id": "gJ7MO-ujIgyv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vmf6nqcIgyv"
      },
      "source": [
        "#### Statistical Parity (Demographic Parity) **5 points**\n",
        "A classifier satisfies this definition if subjects in both protected and unprotected groups have equal probability of being assigned to the positive predicted class."
      ],
      "id": "_Vmf6nqcIgyv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21-YzhbaIgyv"
      },
      "source": [
        "$$P(y_{pred} = 1|G = male) = P(y_{pred} = 1|G = female )$$"
      ],
      "id": "21-YzhbaIgyv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrnmJhUnIgyv"
      },
      "source": [
        "Recall the formula for conditional probability: $P(B|A)=\\frac{P(A \\cap B)}{P(A)}$"
      ],
      "id": "UrnmJhUnIgyv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GiQDHgf3Igyv"
      },
      "outputs": [],
      "source": [
        "def statistical_parity(df):\n",
        "    \"\"\"\n",
        "    TODO: Add your code here\n",
        "    \"\"\"\n",
        "\n",
        "    print('Female Probability of Positive Predictions: %.3f' % female_positive_prob)\n",
        "    print('Male Probability of Positive Predictions: %.3f' % male_positive_prob)\n",
        "    print('Achieves Statistical Parity: %r' % (female_positive_prob == male_positive_prob))"
      ],
      "id": "GiQDHgf3Igyv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SM5xBEDRIgyv"
      },
      "outputs": [],
      "source": [
        "statistical_parity(fair_df)"
      ],
      "id": "SM5xBEDRIgyv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53HK_IFRIgyw"
      },
      "source": [
        "#### Predictive Value Parity **5 points**\n",
        "Positive predictive value (PPV): the fraction of positive cases correctly predicted to be in the positive class out of all predicted positive cases.\n",
        "$$\\frac{TP}{TP+FP}$$\n",
        "A classifier satisfies this definition if both protected and unprotected groups have equal PPV – the probability of a subject with positive predictive value to truly belong to the positive class.\n",
        "$$ P(y_{true}=1|y_{pred} = 1,G = male) = P(y_{true} = 1|y_{pred} = 1,G = female ) $$"
      ],
      "id": "53HK_IFRIgyw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SC6xkQKWIgyw"
      },
      "outputs": [],
      "source": [
        "def predictive_parity(df):\n",
        "    \"\"\"\n",
        "    TODO: Add your code here\n",
        "    \"\"\"\n",
        "\n",
        "    print('Female Probability of True Positive Predictions: %.3f' % PPV_female)\n",
        "    print('Male Probability of True Positive Predictions: %.3f' % PPV_male)\n",
        "    print('Achieves Statistical Parity: %r' % (PPV_female == PPV_male))"
      ],
      "id": "SC6xkQKWIgyw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xQNAaumYIgyw"
      },
      "outputs": [],
      "source": [
        "predictive_parity(fair_df)"
      ],
      "id": "xQNAaumYIgyw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW84QjiTIgyw"
      },
      "source": [
        "#### Equalized Odds (Error Rate Balance) **5 points**\n",
        "**False Positive Error Rate Balance**: A classifier satisfies this definition if both protected and unprotected groups have equal False Positive Rate (FPR) – the probability of a subject in the negative class to have a positive predictive value.\n",
        "$$P(y_{pred} = 1|y_{true} = 0,G = male) = P(y_{pred} = 1|y_{true} = 0,G = female) $$\n",
        "**False Negative Error Rate Balance (Equal Opportunity)**: A classifier satisfies this definition if both protected and unprotected groups have equal False Negative Rate (FNR) – the probability of a subject in a positive class to have a negative predictive value.\n",
        "$$P(y_{pred} = 0|y_{true} = 1,G = male) = P(y_{pred} = 0|y_{true} = 1,G = female)$$\n",
        "The definition of equalized odds combines the previous two: a classifier satisfies the definition if protected and unprotected groups have equal True Positive Rate (TPR) and equal FPR. Mathematically, it is equivalent to the conjunction of conditions for false positive error rate balance and false negative error rate balance definitions given above.\n",
        "$$P(y_{pred} = 1|y_{true} = i,G = male) = P(y_{pred}=1|y_{true}=i,G=female),i \\in 0,1$$"
      ],
      "id": "EW84QjiTIgyw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QhSgwOosIgyw"
      },
      "outputs": [],
      "source": [
        "def equalized_odds(df):\n",
        "    \"\"\"\n",
        "    TODO: Add your code here\n",
        "    \"\"\"\n",
        "\n",
        "    print('Probability of Credit-Worthy Female Predicted Not Credit-Worthy: %.3f' % fnr_female)\n",
        "    print('Probability of Credit-Worthy Male Predicted Not Credit-Worthy: %.3f' % fnr_male)\n",
        "    print('Achieves Equality of Non Credit Worthy Prediction: %r' % (fnr_female == fnr_male))\n",
        "    print('Probability of Non Credit-Worthy Female Predicted Credit-Worthy: %.3f' % fpr_female)\n",
        "    print('Probability of Non Credit-Worthy Male Predicted Credit-Worthy: %.3f' % fpr_male)\n",
        "    print('Achieves Equality of Credit Worthy Prediction: %r' % (fpr_female == fpr_male))"
      ],
      "id": "QhSgwOosIgyw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q3yjoIXWIgyw"
      },
      "outputs": [],
      "source": [
        "equalized_odds(fair_df)"
      ],
      "id": "q3yjoIXWIgyw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WXbvHulIgyx"
      },
      "source": [
        "#### Accuracy Equality **5 points**\n",
        "\n",
        "A classifier satisfies this definition if both protected and unprotected groups have equal prediction accuracy – the probability of a subject from either positive or negative class to be assigned to its respective class. The definition assumes that true negatives are as desirable as true positives."
      ],
      "id": "3WXbvHulIgyx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XNYE0sDVIgyx"
      },
      "outputs": [],
      "source": [
        "def accuracy_equality(df):\n",
        "    \"\"\"\n",
        "    TODO: Add your code here\n",
        "    \"\"\"\n",
        "    print('Female Accuracy: %.3f' % accuracy_female)\n",
        "    print('Male Accuracy: %.3f' % accuracy_male)\n",
        "    print('Equality of Accuracy: %r' % (accuracy_female == accuracy_male))"
      ],
      "id": "XNYE0sDVIgyx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6eTVMEYUIgyx"
      },
      "outputs": [],
      "source": [
        "accuracy_equality(fair_df)"
      ],
      "id": "6eTVMEYUIgyx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoRTz4-OIgy2"
      },
      "source": [
        "#### Treatment Equality **5 points**\n",
        "This definition looks at the ratio of errors that the classifier makes rather than at its accuracy. A\n",
        "classifier satisfies this definition if both protected and unprotected groups have an equal ratio of false negatives and false positives.\n",
        "\n",
        "$$\\frac{FN_{male}}{FP_{male}}=\\frac{FN_{female}}{FP_{female}}$$"
      ],
      "id": "PoRTz4-OIgy2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3lANP6dXIgy2"
      },
      "outputs": [],
      "source": [
        "def treatment_equality(df):\n",
        "    \"\"\"\n",
        "    TODO: Add your code here\n",
        "    \"\"\"\n",
        "\n",
        "    print('Female Ratio of Errors: %.3f' % ratio_female)\n",
        "    print('Male Ratio of Errors: %.3f' % ratio_male)\n",
        "    print('Achieves Treatment Equality: %r' % (ratio_female == ratio_male))"
      ],
      "id": "3lANP6dXIgy2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T4lpnurKIgy2"
      },
      "outputs": [],
      "source": [
        "treatment_equality(fair_df)"
      ],
      "id": "T4lpnurKIgy2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmYMg7bvIgy3"
      },
      "source": [
        "## Q3 Mitigating Algorithmic Bias through Pre-Processing and Post-Processing\n",
        "Satisfying algorithmic fairness definitions can be done through a variety of methods, including pre-, in-, and post-processing. In this homework, we will explore pre-processing and post-processing as approaches to mitigate algorthmic bias."
      ],
      "id": "fmYMg7bvIgy3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX-LMGjLIgy3"
      },
      "source": [
        "### Pre-Processing\n",
        "We will start with pre-processing methods. Pre-processing methods alter the dataset in order to satisfy one or more fairness metrics."
      ],
      "id": "kX-LMGjLIgy3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckrONM9pIgy3"
      },
      "source": [
        "#### Fairness through Unawareness **5 points**\n",
        "Fairness through unawareness is an approach to fairness mitigation in which protected/sensitive attributes are not used to train the model. Run the model without the feature for **sex**. This approach is known as fairness through unwareness.\n",
        "\n",
        "Source: [Fairness Through Unwareness](https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/fairness-criteria/MITRES_EC001S19_video6.pdf)"
      ],
      "id": "ckrONM9pIgy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "efAUBbWsIgy3"
      },
      "outputs": [],
      "source": [
        "# preprocessing code for Taiwan Default dataset\n",
        "\n",
        "def load_dataset_unawareness(filename='default of credit card clients.xls'):\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(filename, header=1)\n",
        "\n",
        "    '''\n",
        "    TODO: Add your code here to drop the 'sex' feature\n",
        "    '''\n",
        "\n",
        "    # Return processed training and testing sets along with gender attributes\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, gender_train.values, gender_test.values\n",
        "\n",
        "# preprocess and load the data\n",
        "X_train, X_test, y_train, y_test, gender_train, gender_test = load_dataset_unawareness()\n",
        "\n",
        "# train a model and obtain predictions on the test set\n",
        "y_pred, y_pred_proba = train_and_predict_model(X_train, X_test, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(accuracy, auc)\n",
        "\n",
        "# Female = 1 and Male = 0\n",
        "fair_df = pd.DataFrame({'sex': gender_test, 'y_true': y_test, 'y_pred': y_pred})\n",
        "fair_df['confusion_matrix'] = fair_df[['y_true','y_pred']].apply(determine_confusion_matrix, axis=1)"
      ],
      "id": "efAUBbWsIgy3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pDMAENvIgy3"
      },
      "source": [
        "Use the fairness measurement functions you created to measure the fairness of this method."
      ],
      "id": "2pDMAENvIgy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze2VX9vQIgy3"
      },
      "outputs": [],
      "source": [
        "statistical_parity(fair_df)\n",
        "predictive_parity(fair_df)\n",
        "equalized_odds(fair_df)\n",
        "accuracy_equality(fair_df)\n",
        "treatment_equality(fair_df)"
      ],
      "id": "Ze2VX9vQIgy3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJB6UhdKIgy3"
      },
      "source": [
        "#### Reweighing **5 points**\n",
        "Reweighing assigns weights for each tuple in the dataset. For those with marginalized protected attributes, positive outcomes will receive greater weights than negative outcomes. For those with non-marginalized sensitive attributes, negative outcomes will receive greater weights than positive outcomes. We assume that we want to remove all discrimination while maintaining the overall positive class probability.\n",
        "\n",
        "For $s \\in \\{m,f\\}$ and $c \\in \\{0,1\\}$, we define the 'unbiased' weights as:\n",
        "$$W(s,c)=\\frac{|X(S)=s| \\times |X(C)=c|}{|D| \\times | X(S)=s \\cap X(C)=c|}$$\n",
        "where $W$ is the reweight function, $D$ is the dataset, $X \\in D$ represents a point in the dataset, $c \\in C$ represents the classes, and $s \\in S$ represents the sensitive attribute of interest.\n",
        "\n",
        "In our case, we should have 4 different weights.\n",
        "\n",
        "Source: [Data preprocessing techniques for classification without discrimination](https://core.ac.uk/download/pdf/81728147.pdf) - See Algorithm 3"
      ],
      "id": "dJB6UhdKIgy3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQdWwpUpIgy3"
      },
      "source": [
        "**Reweigh the dataset and then create a model for the new dataset.**"
      ],
      "id": "JQdWwpUpIgy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "frAuqEyXIgy3"
      },
      "outputs": [],
      "source": [
        "def calculate_weights(df):\n",
        "    # variables to reflect the number of certain attributes\n",
        "    # add other variables if desired\n",
        "    dataset_size = 6000\n",
        "    male_size = 2406\n",
        "    female_size = 3594\n",
        "\n",
        "\n",
        "    \"\"\"TODO: add your code here\"\"\"\n",
        "    # To determine the negative variables (i.e., y = 0), simply subtract\n",
        "    # the following variables from male_size or female_size\n",
        "    male_1_size =\n",
        "    female_1_size =\n",
        "    male_0_size = male_size - male_1_size\n",
        "    female_0_size = female_size - female_1_size\n",
        "\n",
        "    # reweighting scheme for each scenario\n",
        "    w_male_0 =\n",
        "    w_male_1 =\n",
        "    w_female_0 =\n",
        "    w_female_1 =\n",
        "\n",
        "    return w_male_0, w_male_1, w_female_0, w_female_1"
      ],
      "id": "frAuqEyXIgy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PAbU6NcVIgy3"
      },
      "outputs": [],
      "source": [
        "def reweigh_df(df):\n",
        "    \"\"\"\n",
        "    Multiply each reweighting scheme by the appropriate subset of the dataset\n",
        "    \"\"\"\n",
        "    # Get the reweighting scheme\n",
        "    w_male_0, w_male_1, w_female_0, w_female_1 = calculate_weights(df)\n",
        "\n",
        "    '''\n",
        "    TODO: add code here\n",
        "    '''\n",
        "\n",
        "    df['weight'] =\n",
        "\n",
        "    return df"
      ],
      "id": "PAbU6NcVIgy3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0udlB4W0Igy4"
      },
      "outputs": [],
      "source": [
        "# Female = 1 and Male = 0\n",
        "X_train, X_test, y_train, y_test, gender_train, gender_test = load_dataset()\n",
        "df_weight = reweigh_df(pd.DataFrame({'sex': gender_train, 'y_true': y_train}))\n",
        "print(df_weight['weight'].value_counts())\n",
        "y_pred, y_pred_proba = train_and_predict_model(X_train, X_test, y_train, df_weight['weight'])\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(accuracy, auc)\n",
        "\n",
        "fair_df = pd.DataFrame({'sex': gender_test, 'y_true': y_test, 'y_pred': y_pred})\n",
        "fair_df['confusion_matrix'] = fair_df[['y_true','y_pred']].apply(determine_confusion_matrix, axis=1)"
      ],
      "id": "0udlB4W0Igy4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb0_jabhIgy4"
      },
      "source": [
        "Use the fairness measurement functions you created to measure the fairness of this method."
      ],
      "id": "Nb0_jabhIgy4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b49rS4iIgy4"
      },
      "outputs": [],
      "source": [
        "statistical_parity(fair_df)\n",
        "predictive_parity(fair_df)\n",
        "equalized_odds(fair_df)\n",
        "accuracy_equality(fair_df)\n",
        "treatment_equality(fair_df)"
      ],
      "id": "2b49rS4iIgy4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJUCEjHvid0F"
      },
      "id": "lJUCEjHvid0F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmhXKUeeIgy4"
      },
      "source": [
        "#### Relabeling **5 points**\n",
        "Another approach to pre-processing is relabeling. Relabeling changes the labels of some objects in the dataset in order to attempt to remove the discrimination from the input data.\n",
        "\n",
        "One way to relabel the data is to identify likely unfair/discriminatory decisions and correct these by changing the outcome to what ought to have happened. The other way is to change the labeled sensitive class rather than the outcome, and this can be done either randomly or in a systematic way to correct discrimination.\n",
        "\n",
        "Let's keep it simple. Randomly reassign the sensitive attribute **sex** of each individual data point and create a model for this new dataset."
      ],
      "id": "zmhXKUeeIgy4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sv8lh-48Igy4"
      },
      "outputs": [],
      "source": [
        "# preprocessing code for Taiwan Default dataset\n",
        "\n",
        "def load_dataset_relabel(filename='default of credit card clients.xls'):\n",
        "    # Load the dataset\n",
        "    df = pd.read_excel(filename, header=1)\n",
        "\n",
        "    '''\n",
        "    TODO: Add code here to randomly reassign 'sex' of each individual datapoint\n",
        "    '''\n",
        "\n",
        "    # Return processed training and testing sets along with gender attributes\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, gender_train.values, gender_test.values\n",
        "\n",
        "# preprocess and load the data\n",
        "X_train, X_test, y_train, y_test, gender_train, gender_test = load_dataset_relabel()\n",
        "\n",
        "# train a model and obtain predictions on the test set\n",
        "y_pred, y_pred_proba = train_and_predict_model(X_train, X_test, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(accuracy, auc)\n",
        "\n",
        "# Female = 1 and Male = 0\n",
        "fair_df = pd.DataFrame({'sex': gender_test, 'y_true': y_test, 'y_pred': y_pred})\n",
        "fair_df['confusion_matrix'] = fair_df[['y_true','y_pred']].apply(determine_confusion_matrix, axis=1)"
      ],
      "id": "sv8lh-48Igy4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Vm2vO4Igy4"
      },
      "source": [
        "Use the fairness measurement functions you created to measure the fairness of this method."
      ],
      "id": "U8Vm2vO4Igy4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6h9biyxIgy4"
      },
      "outputs": [],
      "source": [
        "statistical_parity(fair_df)\n",
        "predictive_parity(fair_df)\n",
        "equalized_odds(fair_df)\n",
        "accuracy_equality(fair_df)\n",
        "treatment_equality(fair_df)"
      ],
      "id": "i6h9biyxIgy4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}